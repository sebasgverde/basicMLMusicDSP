{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Machine Learning for Music DSP\n",
    "Sebastián García Valencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The field music DSP is a passionate world where converge many areas like physics, math, engineering, music and computer science. The technics can be used in music information retrieval to make really interesting applications. Besides, the last years we have seen a massive interest in general for machine learning and artificial intelligence, the variety of problems of all conceivable areas that this can resolve is very striking.\n",
    "\n",
    "The purpose here is showing some of the basics of this two fields, and demonstrate what we can achieve when combining them. At the end of this notebook you will be able of:\n",
    "\n",
    "- Generate sine waves and export them as wav files\n",
    "- Transform raw audio to spectrograms and analyze them\n",
    "- Generate image databases of spectrograms\n",
    "- Apply a binary classification that recognizes sound and silence\n",
    "- Apply a multiclass classification for the musical notes\n",
    "- Export the trained models to reuse them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sine waves\n",
    "\n",
    "One of the fundamental concepts in audio (and digital signal processing in general) is the sine wave. When you study the field more in depth, you will know that every signal possible can be modelled as a sum of sine waves with different frequencies, amplitudes and phases. Specifically, these 3 parameters constitute the sine wave formula (which varies over time)\n",
    "\n",
    "$$y(t) = Asin(2{\\pi}f + \\phi$$  where A is the amplitude, f is the frequency and $\\phi$ is the phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start defining 2 methods, one to create a signal made of a sine wave of one second and a silence of one second. The other to take a sine wave and export it as a wav sound file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "num_samples = 41100\n",
    "# The sampling rate of the analog to digital convert\n",
    "sampling_rate = 41100.0\n",
    "amplitude = 16000\n",
    "# creates a sine wave with the given frequence of one seconde and a silence of one second\n",
    "def make_sound_and_silence(freq=1000):\n",
    "    # frequency is the number of times a wave repeats a second\n",
    "    frequency = freq\n",
    "    sine_wave_part = [np.sin(2 * np.pi * frequency * x/sampling_rate) for x in range(num_samples)] + [0] * num_samples\n",
    "    return sine_wave_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nframes=num_samples\n",
    "comptype=\"NONE\"\n",
    "compname=\"not compressed\"\n",
    "nchannels=1\n",
    "sampwidth=2\n",
    "\n",
    "# recieves and array with a sine wave and creates a wav file\n",
    "def save_wav(sine_wave, file = \"test.wav\"):\n",
    "    wav_file=wave.open(file, 'w')\n",
    "    wav_file.setparams((nchannels, sampwidth, int(sampling_rate), nframes, comptype, compname))\n",
    "    for s in sine_wave:\n",
    "       wav_file.writeframes(struct.pack('h', int(s*amplitude)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a sine wave with a frequency of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_sine_wave = make_sound_and_silence(1000)\n",
    "\n",
    "print(little_sine_wave[:10])\n",
    "print(len(little_sine_wave))\n",
    "\n",
    "save_wav(little_sine_wave, \"shorttest.wav\")\n",
    "print(\"ready!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# We'll need numpy for some mathematical operations\n",
    "import numpy as np\n",
    "# matplotlib for displaying the output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "# and IPython.display for audio output\n",
    "import IPython.display\n",
    "# Librosa for audio\n",
    "import librosa\n",
    "# And the display module for visualization\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's listen to the wave and visualize the first millisecond, with a sample rate of 41100 (samples per second) each millisecond will have 411 samples. The frequency of 1000 means, the sine wave makes a complete oscillation 1000 times per second, so in this first millisecond, there should be 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example track\n",
    "y, sr = librosa.load(\"shorttest.wav\")\n",
    "\n",
    "# Play it back!\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(range(len(little_sine_wave[:411])),little_sine_wave[:411])\n",
    "\n",
    "IPython.display.Audio(data=y, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the complete wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(range(len(little_sine_wave)),little_sine_wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be generating a signal made of sounds and silences, always with frequencies multiple of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_wave=[]\n",
    "for i in range(1,11):\n",
    "    sine_wave += make_sound_and_silence(500*i)\n",
    "\n",
    "print(sine_wave[:10])\n",
    "print(len(sine_wave))\n",
    "\n",
    "save_wav(sine_wave, \"longtest.wav\")\n",
    "print(\"ready!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(\"longtest.wav\")\n",
    "IPython.display.Audio(data=y, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram\n",
    "\n",
    "At this point, it is necessary to speak about another very important concept un DSP. The spectrogram is a way to represent a signal showing the variation of frequency over time. The usual method to generate it is the fast Fourier transform (FFT) which is out of the scope here, but you should read more about it if you want to go more in depth in the topic. The main idea is to have chunks of time and calculate the predominance of some frequencies with the intensity of colours. Said this, can you imagine how should look the spectrogram of \"longtest.wav\". In theory, it should be almost black in all the frequencies except the respective multiple of 500, this should change every second, and in the silence parts there should be black vertical rectangles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make and display a mel-scaled power (energy-squared) spectrogram\n",
    "S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "# Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
    "log_S = librosa.power_to_db(S, ref=np.max)\n",
    "# Make a new figure\n",
    "plt.figure(figsize=(12,4))\n",
    "# Display the spectrogram on a mel scale\n",
    "# sample rate and hop length parameters are used to render the time axis\n",
    "librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
    "# Put a descriptive title on the plot\n",
    "plt.title('mel power spectrogram')\n",
    "# draw a color bar\n",
    "plt.colorbar(format='%+02.0f dB')\n",
    "# Make the figure layout compact\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"testSpectrogram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this gives a lot of information and being an image, ideas for using it with machine learning come to the mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "The basic idea in machine learning is creating models which learn by itself the hidden patterns in the data without having to program it explicitly. There many subdivisions like unsupervised learning (clustering, anomaly detection, etc.) and supervised learning. The second has at the same time a distinction depending on if the data is continuous (regression) or categorical (classification), and the idea is giving labels to the data. I'll focus on the classification image problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a dataset with each second of a wav\n",
    "\n",
    "The starting point is to create the data, first, let's take the spectrogram and delete all the extra information like axis, text, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "log_S = librosa.power_to_db(S, ref=np.max)\n",
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.specshow(log_S, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a method that creates a dataset folder with a given wav file, making an image of the spectrogram of each second and using the image name as label container (0 for silence and 1 for sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with each second of a wav\n",
    "def create_dataset(audio_path, dataset_name=\"dataset\"):\n",
    "    \n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    #use sr (sampling rate) to split data in seconds\n",
    "    import os \n",
    "    if not os.path.exists(dataset_name):\n",
    "        os.makedirs(dataset_name)\n",
    "\n",
    "    initial = 0\n",
    "    final = sr\n",
    "    for i in range(len(y)/sr):\n",
    "        # Let's make and display a mel-scaled power (energy-squared) spectrogram\n",
    "        S = librosa.feature.melspectrogram(y[initial:final], sr=sr, n_mels=128)\n",
    "\n",
    "        # Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
    "        log_S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        # Make a new figure\n",
    "        plt.figure(figsize=(12,4))\n",
    "\n",
    "\n",
    "        # Display the spectrogram on a mel scale\n",
    "        # sample rate and hop length parameters are used to render the time axis\n",
    "        librosa.display.specshow(log_S, sr=sr)    \n",
    "\n",
    "        #create dataset with 0 for silence and 1 for sound\n",
    "        if i % 2 == 0:\n",
    "            plt.savefig(dataset_name + \"/second_\"+str(i)+\"_label_1.png\")\n",
    "        else:\n",
    "            plt.savefig(dataset_name + \"/second_\"+str(i)+\"_label_0.png\")\n",
    "        initial += sr\n",
    "        final += sr\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(\"longtest.wav\")\n",
    "\n",
    "print(len(y))\n",
    "print(sr)\n",
    "len(y)/sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "This problem (classificate as silence or sound) is known as binary classfication, I'll use the simplest calssification model and also, the canonic election for binary classification: logistic regresion\n",
    "\n",
    "The first step is to prepare the data in the format of flat matrix representations of the images (data), the images itself (images) and the labels obtained from the image file name (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "def load_data(folder = \"dataset\"):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for image in os.listdir(folder):\n",
    "        print(image)\n",
    "        images.append(plt.imread(folder + \"/\" + image))\n",
    "        labels.append(int(image.split(\"_\")[3].split(\".\")[0]))\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    print(labels)\n",
    "    images.shape\n",
    "\n",
    "    # To apply a classifier on this data, we need to flatten the image, to\n",
    "    # turn the data in a (samples, feature) matrix:\n",
    "    n_samples = len(images)\n",
    "    data = images.reshape((n_samples, -1))\n",
    "    \n",
    "    return data, images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, images, labels = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "The basic idea is to $\\beta_0$ and $\\beta_1$ that optimize the logistic function\n",
    "\n",
    "$$LogisticReg(x)=\\frac{1}{(1+e^{-(\\beta_0+\\beta_1 x)})}$$\n",
    "\n",
    "which has and s form whit a range between 0 and 1 (the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,4))\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard procedure is to split the data into train and test data, the first to train the model and the second to measure is performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(images, labels))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i\\n' % label, fontsize = 20)\n",
    "    \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "\n",
    "predictions = logisticRegr.predict(x_test)\n",
    "print(\"Predictions for test:\")\n",
    "print(predictions)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(\"score:\")\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model achieves a score of 100%. This is cause the problem is very simple, so the model can easily memorize all the possible images (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification (musical notes)\n",
    "\n",
    "The challenge now is classifying the 7 fundamental musical notes starting in central C. This kind of problem is known as multiclass classification.\n",
    "\n",
    "In this case, the procedure changes a little, first let's define a dictionary with the frequencies of each note (using https://pages.mtu.edu/~suits/notefreqs.html beginning with central C (C4)) and another to map from the note name to the label. Then just create a wav, with random selection of notes from the dictionary, in this case, I'll use the file name of the wav to store the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = {\"C\" : 261.63, \"D\" : 293.66, \"E\" : 329.63, \"F\" : 349.23, \"G\" : 392.00, \"A\" : 440.00, \"B\" : 493.88}\n",
    "labels_dict = { \"silence\" : 0,\"C\" : 1, \"D\" : 2, \"E\" : 3, \"F\" : 4, \"G\" : 5, \"A\" : 6, \"B\" : 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sine_wave=[]\n",
    "file_name = \"\"\n",
    "for i in range(0,20):\n",
    "    rand_note = random.choice(notes.keys())\n",
    "    file_name += str(labels_dict[rand_note]) + \"_\"\n",
    "    sine_wave += make_sound_and_silence(notes[rand_note])\n",
    "\n",
    "print(sine_wave[:10])\n",
    "print(len(sine_wave))\n",
    "\n",
    "wav_name = file_name + \"notes.wav\"\n",
    "save_wav(sine_wave, wav_name)\n",
    "print(\"ready!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to create the dataset I'll trust in the order of the notes in the file name which corresponds to the order in wav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with each second of a wav\n",
    "def create_dataset_notes(audio_path, dataset_name=\"notesdataset\"):\n",
    "    \n",
    "    y, sr = librosa.load(audio_path)\n",
    "    \n",
    "    labels = audio_path.split(\"_\")\n",
    "\n",
    "    #use sr (sampling rate) to split data in seconds\n",
    "    import os \n",
    "    if not os.path.exists(dataset_name):\n",
    "        os.makedirs(dataset_name)\n",
    "\n",
    "    initial = 0\n",
    "    final = sr\n",
    "    for i in range(len(y)/sr):\n",
    "        # Let's make and display a mel-scaled power (energy-squared) spectrogram\n",
    "        S = librosa.feature.melspectrogram(y[initial:final], sr=sr, n_mels=128)\n",
    "\n",
    "        # Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
    "        log_S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        # Make a new figure\n",
    "        plt.figure(figsize=(12,4))\n",
    "\n",
    "\n",
    "        # Display the spectrogram on a mel scale\n",
    "        # sample rate and hop length parameters are used to render the time axis\n",
    "        librosa.display.specshow(log_S, sr=sr)    \n",
    "\n",
    "        #create dataset with 0 for silence and 1 for sound\n",
    "        if i % 2 == 0:\n",
    "            plt.savefig(dataset_name + \"/second_\"+str(i)+\"_label_\" + labels[i/2] + \".png\")\n",
    "        else:\n",
    "            # we only want the notes in this case\n",
    "            print(\"silence\")\n",
    "            #plt.savefig(dataset_name + \"/second_\"+str(i)+\"_label_0.png\")\n",
    "        initial += sr\n",
    "        final += sr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_dataset_notes(wav_name, \"datasetnotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, images, labels = load_data(\"datasetnotes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) \n",
    "\n",
    "With random forest and neural networks, support vector machines are one of the most potent methods for classification. The basic idea of SVM is to generate hyperplanes in multidimensional spaces which separates the different categories into groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_and_labels = list(zip(images, labels))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "    \n",
    "n_samples = len(images)\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifierSVM = svm.SVC(gamma=0.001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifierSVM.fit(data[:n_samples // 2], labels[:n_samples // 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = labels[n_samples // 2:]\n",
    "predicted = classifierSVM.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifierSVM, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are reported 3 different metrics of the model. The intuitive description is: precision tells how much of what the model predicts is correct, recall tells which portion of all that it was possible to predict in the model was correctly predicted, and the f1-score is a combination of both. The confusions matrix relates the true labels with the predicted, to know in which categories the models tend to be \"confused\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the train models as pickles\n",
    "\n",
    "Finally, let's export the trained models as pickle files, this will be the usual procedure to deploy the model. Later, the model just has to be loaded and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(logisticRegr, open('logisticRegr.p','wb'))\n",
    "pickle.dump(classifierSVM, open('classifierSVM.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_logreg = pickle.load(file('logisticRegr.p'))\n",
    "print(\"ready!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_logreg.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_svm = pickle.load(file('classifierSVM.p'))\n",
    "print(\"ready!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_svm.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some sources\n",
    "\n",
    "- [https://github.com/librosa/librosa](https://github.com/librosa/librosa)\n",
    "- [https://magenta.tensorflow.org/onsets-frames](https://magenta.tensorflow.org/onsets-frames)\n",
    "- [http://pythonforengineers.com/audio-and-digital-signal-processingdsp-in-python/](http://pythonforengineers.com/audio-and-digital-signal-processingdsp-in-python/)\n",
    "- [https://en.wikipedia.org/wiki/Sine_wave](https://en.wikipedia.org/wiki/Sine_wave)\n",
    "- [https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a](https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a)\n",
    "- [https://pages.mtu.edu/~suits/notefreqs.html](https://pages.mtu.edu/~suits/notefreqs.html)\n",
    "- [http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
